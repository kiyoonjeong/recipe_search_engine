{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recipe_preprocess",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWaDlfxZsQ6-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import sklearn\n",
        "import ast\n",
        "import re\n",
        "df = open(\"/content/drive/MyDrive/raw-data_recipe.csv\", encoding='UTF8')\n",
        "read_csv = pd.read_csv(df)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD9gJZY9wOIp"
      },
      "source": [
        "read_csv = read_csv.drop(columns=['reviews', 'recipe_id', 'image_url'])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGuQeQmZ6uUp"
      },
      "source": [
        "def time_to_num(time_var):\n",
        "  time_arr = re.split(' ', time_var)\n",
        "  total_time = 0\n",
        "  time_type = set(['m','h','d'])\n",
        "  for element in time_arr:\n",
        "    if element not in time_type:\n",
        "      time = int(element)\n",
        "    else:\n",
        "      if element == 'd':\n",
        "        total_time = 24*60*time\n",
        "      elif element == 'h':\n",
        "        total_time = 60*time\n",
        "      else:\n",
        "        total_time += time\n",
        "\n",
        "  return total_time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdrauNRC7SWQ"
      },
      "source": [
        "def get_time(element):\n",
        "  res = ast.literal_eval(element)\n",
        "  arr = re.split('\\n', res['directions'])\n",
        "  if len(arr) > 5 and arr[4] == 'Ready In':\n",
        "    time = time_to_num(arr[5])\n",
        "    direction = ' '.join(arr[6:])\n",
        "  elif len(arr) > 4 and arr[0] == 'Prep':\n",
        "    time = time_to_num(arr[1]) + time_to_num(arr[3])\n",
        "    direction = ' '.join(arr[4:])\n",
        "  elif arr[0] == 'Cook':\n",
        "    time = time_to_num(arr[3])\n",
        "    direction = ' '.join(arr[4:])\n",
        "  elif arr[0] == 'Ready In':\n",
        "    time = time_to_num(arr[1])\n",
        "    direction = ' '.join(arr[2:])\n",
        "  elif arr[0] == 'Prep' :\n",
        "    time = -1\n",
        "    direction = 'None'\n",
        "  else:\n",
        "    time = -1\n",
        "    direction = ' '.join(arr)\n",
        "  return time, direction"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGtgZKbX635-"
      },
      "source": [
        "read_csv[\"time\"] = read_csv[\"cooking_directions\"].apply(lambda x : get_time(x)[0])\n",
        "read_csv[\"recipe\"] = read_csv[\"cooking_directions\"].apply(lambda x : get_time(x)[1])\n",
        "read_csv = read_csv.drop(columns=['cooking_directions'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkGvK6fAEl3H",
        "outputId": "0d4e2b82-ea57-4253-c3f6-af065e2a282f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "wnl = nltk.WordNetLemmatizer()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B8X_K1E_ENr"
      },
      "source": [
        "def word_tokenize(element):\n",
        "  x = element.replace('^', ' ').replace('(', ' ').replace(')', ' ').replace('®','').lower()\n",
        "  arr = [wnl.lemmatize(word) for word in nltk.wordpunct_tokenize(x)]\n",
        "  return ' '.join(arr)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbyDzA9aLL0I"
      },
      "source": [
        "read_csv['name'] = read_csv['recipe_name'].apply(lambda x: word_tokenize(x))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWzYd4Pf_cSR"
      },
      "source": [
        "read_csv[\"ingredients_token\"] = read_csv[\"ingredients\"].apply(lambda x : word_tokenize(x))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTAHJUOsWHAd"
      },
      "source": [
        "read_csv[\"tool\"] = read_csv[\"recipe\"].apply(lambda x : word_tokenize(x))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6ZDPmrRYk3B"
      },
      "source": [
        "read_csv[\"ing_tool\"] = read_csv[['ingredients_token','tool']].agg(' '.join, axis=1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C__6VQuwaUJB"
      },
      "source": [
        "read_csv = read_csv.drop(columns=['tool', 'ingredients_token'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRQY-QxGYF3A"
      },
      "source": [
        "read_csv[\"tokens\"] = read_csv[['name','ing_tool']].agg(' '.join, axis=1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAbABtjtZp6j"
      },
      "source": [
        "def str_set(element):\n",
        "  return set(re.split(' ', element))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUoVbekBaDfL"
      },
      "source": [
        "read_csv['name'] = read_csv['name'].apply(lambda x: str_set(x))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT0Niz0BaLr4"
      },
      "source": [
        "read_csv[\"ing_tool\"] = read_csv[\"ing_tool\"].apply(lambda x : str_set(x))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgOhgiQemJPc"
      },
      "source": [
        "nut_dict = {'sugars':0, 'fat':0, 'calories':0, 'sodium':0}\n",
        "for element in read_csv.nutritions:\n",
        "  res = ast.literal_eval(element)\n",
        "  if res['sugars']['name']:\n",
        "    nut_dict['sugars'] += res['sugars']['amount']\n",
        "  if res['fat']['name']:\n",
        "    nut_dict['fat']  += res['fat']['amount']\n",
        "  if res['calories']['name']:\n",
        "    nut_dict['calories']  += res['calories']['amount']\n",
        "  if res['sodium']['name']:\n",
        "    nut_dict['sodium'] += res['sodium']['amount']"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AiEjwffeJer"
      },
      "source": [
        "def low_check(element, nutrition):\n",
        "  res = ast.literal_eval(element)\n",
        "  if res[nutrition]['name'] and nut_dict[nutrition] > res[nutrition]['amount']*len_df:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrWHBdPJgZyJ"
      },
      "source": [
        "len_df = len(read_csv)\n",
        "for nut in nut_dict.keys():\n",
        "  read_csv[nut] = read_csv[\"nutritions\"].apply(lambda x : low_check(x, nut))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkv6YAVVn5qe"
      },
      "source": [
        "# In average, 20 ingredients are used in each dish. \n",
        "# Similar : 100 among 50000 -> 500 topics -> 200 topics -> 100 topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG3AzA-Iu9lx",
        "outputId": "4d6f88b5-0b02-4498-c57d-d49dc78e14a3"
      },
      "source": [
        "count = 0\n",
        "for element in read_csv['tokens']:\n",
        "  count += len(re.split(' ', element))\n",
        "print(count//len_df)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_65ZO_sesfkN"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_df = 0.95, stop_words ='english')\n",
        "cv_text = cv.fit_transform(read_csv['tokens'])\n",
        "features = cv.get_feature_names()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mY6gIt1s84P",
        "outputId": "b084221b-f2b9-46a1-8c7b-e884c0f651be"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components = 100)\n",
        "lda.fit(cv_text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                          evaluate_every=-1, learning_decay=0.7,\n",
              "                          learning_method='batch', learning_offset=10.0,\n",
              "                          max_doc_update_iter=100, max_iter=10,\n",
              "                          mean_change_tol=0.001, n_components=100, n_jobs=None,\n",
              "                          perp_tol=0.1, random_state=None,\n",
              "                          topic_word_prior=None, total_samples=1000000.0,\n",
              "                          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37zdusG8tUdh"
      },
      "source": [
        "# for index, topic in enumerate(lda.components_):\n",
        "#     print('Top 10 ingredients for topic',index)\n",
        "#     print([cv.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
        "#     print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eJUQ43euIqy"
      },
      "source": [
        "test = lda.transform(cv_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-wzWjBOM_S7"
      },
      "source": [
        "temp = []\n",
        "for element in test:\n",
        "    temp.append(np.argmax(element))\n",
        "topic_class = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUaqeLtsU4ZK"
      },
      "source": [
        "read_csv['topic'] = topic_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifsz3tyVAG-z"
      },
      "source": [
        "read_csv = read_csv.drop(columns=['nutritions'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWSerecSbYQt"
      },
      "source": [
        "read_csv.to_csv(\"/content/drive/My Drive/recipe_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiwQ2qlm8Chc"
      },
      "source": [
        "token_list = set()\n",
        "\n",
        "for element in read_csv['ingredients']:\n",
        "  x = element.replace('^', ' ').replace('(', ' ').replace(')', ' ').replace('®','').lower()\n",
        "  arr = [wnl.lemmatize(word) for word in nltk.wordpunct_tokenize(x)]\n",
        "  for ing in arr:\n",
        "    token_list.add(ing)\n",
        "for element in read_csv[\"recipe\"]:\n",
        "  x = ' '.join(re.split(r\"\\W+\", element)).lower()\n",
        "  arr = [wnl.lemmatize(word) for word in nltk.wordpunct_tokenize(x)]\n",
        "  for tool in arr:\n",
        "    token_list.add(tool)\n",
        "for element in read_csv['recipe_name']:\n",
        "  x = element.replace('^', ' ').replace('(', ' ').replace(')', ' ').replace('®','').lower()\n",
        "  arr = [wnl.lemmatize(word) for word in nltk.wordpunct_tokenize(x)]\n",
        "  for ing in arr:\n",
        "    token_list.add(ing)\n",
        "token_list = pd.DataFrame(token_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14GZO-5k3-Mq"
      },
      "source": [
        "token_list.to_csv(\"/content/drive/My Drive/token_list.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}